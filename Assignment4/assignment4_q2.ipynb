{
  "cells": [
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\niris=pd.read_csv(\"iris.csv\")",
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Shuffling the data for improving the results\niris = iris.sample(frac=1).reset_index(drop=True)\nprint(iris)",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": "     sepal.length  sepal.width  petal.length  petal.width     variety\n0             7.2          3.6           6.1          2.5   Virginica\n1             7.3          2.9           6.3          1.8   Virginica\n2             7.2          3.2           6.0          1.8   Virginica\n3             5.1          3.8           1.9          0.4      Setosa\n4             5.3          3.7           1.5          0.2      Setosa\n5             6.1          3.0           4.6          1.4  Versicolor\n6             6.0          2.2           5.0          1.5   Virginica\n7             5.0          3.5           1.6          0.6      Setosa\n8             6.8          3.0           5.5          2.1   Virginica\n9             4.6          3.4           1.4          0.3      Setosa\n10            4.4          3.0           1.3          0.2      Setosa\n11            5.8          2.7           4.1          1.0  Versicolor\n12            6.3          2.5           5.0          1.9   Virginica\n13            5.4          3.0           4.5          1.5  Versicolor\n14            5.7          4.4           1.5          0.4      Setosa\n15            5.7          2.6           3.5          1.0  Versicolor\n16            5.1          3.8           1.6          0.2      Setosa\n17            5.4          3.7           1.5          0.2      Setosa\n18            5.0          3.6           1.4          0.2      Setosa\n19            7.7          2.8           6.7          2.0   Virginica\n20            6.7          3.3           5.7          2.1   Virginica\n21            5.0          2.3           3.3          1.0  Versicolor\n22            6.7          2.5           5.8          1.8   Virginica\n23            4.9          3.1           1.5          0.2      Setosa\n24            4.6          3.1           1.5          0.2      Setosa\n25            5.0          3.5           1.3          0.3      Setosa\n26            4.8          3.4           1.6          0.2      Setosa\n27            5.0          3.4           1.5          0.2      Setosa\n28            7.1          3.0           5.9          2.1   Virginica\n29            6.1          2.6           5.6          1.4   Virginica\n..            ...          ...           ...          ...         ...\n120           6.5          2.8           4.6          1.5  Versicolor\n121           6.0          2.7           5.1          1.6  Versicolor\n122           6.5          3.2           5.1          2.0   Virginica\n123           5.4          3.4           1.7          0.2      Setosa\n124           4.9          3.0           1.4          0.2      Setosa\n125           5.7          2.9           4.2          1.3  Versicolor\n126           5.1          3.8           1.5          0.3      Setosa\n127           5.1          3.4           1.5          0.2      Setosa\n128           6.4          3.2           5.3          2.3   Virginica\n129           6.7          3.3           5.7          2.5   Virginica\n130           5.2          2.7           3.9          1.4  Versicolor\n131           5.8          2.7           5.1          1.9   Virginica\n132           6.2          2.2           4.5          1.5  Versicolor\n133           6.2          2.9           4.3          1.3  Versicolor\n134           5.2          3.5           1.5          0.2      Setosa\n135           7.2          3.0           5.8          1.6   Virginica\n136           6.3          3.3           4.7          1.6  Versicolor\n137           6.4          3.1           5.5          1.8   Virginica\n138           7.7          2.6           6.9          2.3   Virginica\n139           5.5          2.5           4.0          1.3  Versicolor\n140           4.9          2.5           4.5          1.7   Virginica\n141           5.8          2.6           4.0          1.2  Versicolor\n142           5.9          3.0           5.1          1.8   Virginica\n143           5.7          2.8           4.5          1.3  Versicolor\n144           5.5          3.5           1.3          0.2      Setosa\n145           6.4          2.9           4.3          1.3  Versicolor\n146           6.1          3.0           4.9          1.8   Virginica\n147           6.7          3.0           5.0          1.7  Versicolor\n148           7.9          3.8           6.4          2.0   Virginica\n149           5.0          3.0           1.6          0.2      Setosa\n\n[150 rows x 5 columns]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "X = iris[['sepal.length', 'sepal.width', 'petal.length', 'petal.width']]\nX = np.array(X)\nprint(X)\nprint(\"Shape of X\",X.shape)",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[[7.2 3.6 6.1 2.5]\n [7.3 2.9 6.3 1.8]\n [7.2 3.2 6.  1.8]\n [5.1 3.8 1.9 0.4]\n [5.3 3.7 1.5 0.2]\n [6.1 3.  4.6 1.4]\n [6.  2.2 5.  1.5]\n [5.  3.5 1.6 0.6]\n [6.8 3.  5.5 2.1]\n [4.6 3.4 1.4 0.3]\n [4.4 3.  1.3 0.2]\n [5.8 2.7 4.1 1. ]\n [6.3 2.5 5.  1.9]\n [5.4 3.  4.5 1.5]\n [5.7 4.4 1.5 0.4]\n [5.7 2.6 3.5 1. ]\n [5.1 3.8 1.6 0.2]\n [5.4 3.7 1.5 0.2]\n [5.  3.6 1.4 0.2]\n [7.7 2.8 6.7 2. ]\n [6.7 3.3 5.7 2.1]\n [5.  2.3 3.3 1. ]\n [6.7 2.5 5.8 1.8]\n [4.9 3.1 1.5 0.2]\n [4.6 3.1 1.5 0.2]\n [5.  3.5 1.3 0.3]\n [4.8 3.4 1.6 0.2]\n [5.  3.4 1.5 0.2]\n [7.1 3.  5.9 2.1]\n [6.1 2.6 5.6 1.4]\n [5.8 2.8 5.1 2.4]\n [4.8 3.1 1.6 0.2]\n [6.5 3.  5.5 1.8]\n [5.2 3.4 1.4 0.2]\n [5.8 2.7 5.1 1.9]\n [6.4 2.8 5.6 2.1]\n [4.8 3.  1.4 0.1]\n [6.1 2.8 4.  1.3]\n [6.5 3.  5.8 2.2]\n [5.4 3.9 1.3 0.4]\n [6.3 2.8 5.1 1.5]\n [5.9 3.  4.2 1.5]\n [6.3 3.4 5.6 2.4]\n [6.3 2.3 4.4 1.3]\n [5.1 3.5 1.4 0.2]\n [4.6 3.2 1.4 0.2]\n [4.7 3.2 1.3 0.2]\n [6.  3.4 4.5 1.6]\n [5.1 3.5 1.4 0.3]\n [5.7 3.  4.2 1.2]\n [5.4 3.4 1.5 0.4]\n [5.7 2.5 5.  2. ]\n [7.6 3.  6.6 2.1]\n [4.8 3.4 1.9 0.2]\n [7.  3.2 4.7 1.4]\n [4.9 3.1 1.5 0.1]\n [6.4 2.8 5.6 2.2]\n [4.5 2.3 1.3 0.3]\n [5.6 2.8 4.9 2. ]\n [5.  3.2 1.2 0.2]\n [5.5 2.6 4.4 1.2]\n [6.3 2.7 4.9 1.8]\n [6.5 3.  5.2 2. ]\n [6.3 2.5 4.9 1.5]\n [5.8 2.7 3.9 1.2]\n [5.1 2.5 3.  1.1]\n [4.9 3.6 1.4 0.1]\n [6.9 3.1 5.1 2.3]\n [5.2 4.1 1.5 0.1]\n [7.7 3.  6.1 2.3]\n [4.4 3.2 1.3 0.2]\n [5.5 4.2 1.4 0.2]\n [5.1 3.3 1.7 0.5]\n [5.6 3.  4.5 1.5]\n [7.7 3.8 6.7 2.2]\n [5.4 3.9 1.7 0.4]\n [6.  3.  4.8 1.8]\n [6.9 3.2 5.7 2.3]\n [6.2 3.4 5.4 2.3]\n [6.6 2.9 4.6 1.3]\n [6.8 2.8 4.8 1.4]\n [4.4 2.9 1.4 0.2]\n [4.9 2.4 3.3 1. ]\n [5.8 4.  1.2 0.2]\n [6.9 3.1 4.9 1.5]\n [5.9 3.2 4.8 1.8]\n [5.6 2.9 3.6 1.3]\n [5.1 3.7 1.5 0.4]\n [6.2 2.8 4.8 1.8]\n [5.5 2.4 3.8 1.1]\n [5.5 2.4 3.7 1. ]\n [6.1 2.9 4.7 1.4]\n [6.3 3.3 6.  2.5]\n [6.4 3.2 4.5 1.5]\n [6.  2.2 4.  1. ]\n [4.3 3.  1.1 0.1]\n [6.8 3.2 5.9 2.3]\n [6.6 3.  4.4 1.4]\n [5.6 2.5 3.9 1.1]\n [6.  2.9 4.5 1.5]\n [5.7 2.8 4.1 1.3]\n [5.5 2.3 4.  1.3]\n [6.7 3.1 4.7 1.5]\n [4.8 3.  1.4 0.3]\n [6.1 2.8 4.7 1.2]\n [5.  3.3 1.4 0.2]\n [6.7 3.1 4.4 1.4]\n [5.7 3.8 1.7 0.3]\n [4.6 3.6 1.  0.2]\n [7.4 2.8 6.1 1.9]\n [5.6 2.7 4.2 1.3]\n [6.4 2.7 5.3 1.9]\n [6.7 3.1 5.6 2.4]\n [4.7 3.2 1.6 0.2]\n [6.3 2.9 5.6 1.8]\n [5.6 3.  4.1 1.3]\n [6.7 3.  5.2 2.3]\n [6.9 3.1 5.4 2.1]\n [5.  2.  3.5 1. ]\n [5.  3.4 1.6 0.4]\n [6.5 2.8 4.6 1.5]\n [6.  2.7 5.1 1.6]\n [6.5 3.2 5.1 2. ]\n [5.4 3.4 1.7 0.2]\n [4.9 3.  1.4 0.2]\n [5.7 2.9 4.2 1.3]\n [5.1 3.8 1.5 0.3]\n [5.1 3.4 1.5 0.2]\n [6.4 3.2 5.3 2.3]\n [6.7 3.3 5.7 2.5]\n [5.2 2.7 3.9 1.4]\n [5.8 2.7 5.1 1.9]\n [6.2 2.2 4.5 1.5]\n [6.2 2.9 4.3 1.3]\n [5.2 3.5 1.5 0.2]\n [7.2 3.  5.8 1.6]\n [6.3 3.3 4.7 1.6]\n [6.4 3.1 5.5 1.8]\n [7.7 2.6 6.9 2.3]\n [5.5 2.5 4.  1.3]\n [4.9 2.5 4.5 1.7]\n [5.8 2.6 4.  1.2]\n [5.9 3.  5.1 1.8]\n [5.7 2.8 4.5 1.3]\n [5.5 3.5 1.3 0.2]\n [6.4 2.9 4.3 1.3]\n [6.1 3.  4.9 1.8]\n [6.7 3.  5.  1.7]\n [7.9 3.8 6.4 2. ]\n [5.  3.  1.6 0.2]]\nShape of X (150, 4)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Encoding cells as Setosa-0,Versicolor-1,Virginica-2 then to [1,0,0] for Setosa,[0,1,0] for Versicolor,[0,0,1] for Virginica"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.preprocessing import OneHotEncoder\none_hot_encoder = OneHotEncoder(sparse=False)",
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "Y = iris.variety\nprint(Y)\nY = one_hot_encoder.fit_transform(np.array(Y).reshape(-1, 1))\nprint(Y)",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": "0       Virginica\n1       Virginica\n2       Virginica\n3          Setosa\n4          Setosa\n5      Versicolor\n6       Virginica\n7          Setosa\n8       Virginica\n9          Setosa\n10         Setosa\n11     Versicolor\n12      Virginica\n13     Versicolor\n14         Setosa\n15     Versicolor\n16         Setosa\n17         Setosa\n18         Setosa\n19      Virginica\n20      Virginica\n21     Versicolor\n22      Virginica\n23         Setosa\n24         Setosa\n25         Setosa\n26         Setosa\n27         Setosa\n28      Virginica\n29      Virginica\n          ...    \n120    Versicolor\n121    Versicolor\n122     Virginica\n123        Setosa\n124        Setosa\n125    Versicolor\n126        Setosa\n127        Setosa\n128     Virginica\n129     Virginica\n130    Versicolor\n131     Virginica\n132    Versicolor\n133    Versicolor\n134        Setosa\n135     Virginica\n136    Versicolor\n137     Virginica\n138     Virginica\n139    Versicolor\n140     Virginica\n141    Versicolor\n142     Virginica\n143    Versicolor\n144        Setosa\n145    Versicolor\n146     Virginica\n147    Versicolor\n148     Virginica\n149        Setosa\nName: variety, Length: 150, dtype: object\n[[0. 0. 1.]\n [0. 0. 1.]\n [0. 0. 1.]\n [1. 0. 0.]\n [1. 0. 0.]\n [0. 1. 0.]\n [0. 0. 1.]\n [1. 0. 0.]\n [0. 0. 1.]\n [1. 0. 0.]\n [1. 0. 0.]\n [0. 1. 0.]\n [0. 0. 1.]\n [0. 1. 0.]\n [1. 0. 0.]\n [0. 1. 0.]\n [1. 0. 0.]\n [1. 0. 0.]\n [1. 0. 0.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 1. 0.]\n [0. 0. 1.]\n [1. 0. 0.]\n [1. 0. 0.]\n [1. 0. 0.]\n [1. 0. 0.]\n [1. 0. 0.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 0. 1.]\n [1. 0. 0.]\n [0. 0. 1.]\n [1. 0. 0.]\n [0. 0. 1.]\n [0. 0. 1.]\n [1. 0. 0.]\n [0. 1. 0.]\n [0. 0. 1.]\n [1. 0. 0.]\n [0. 0. 1.]\n [0. 1. 0.]\n [0. 0. 1.]\n [0. 1. 0.]\n [1. 0. 0.]\n [1. 0. 0.]\n [1. 0. 0.]\n [0. 1. 0.]\n [1. 0. 0.]\n [0. 1. 0.]\n [1. 0. 0.]\n [0. 0. 1.]\n [0. 0. 1.]\n [1. 0. 0.]\n [0. 1. 0.]\n [1. 0. 0.]\n [0. 0. 1.]\n [1. 0. 0.]\n [0. 0. 1.]\n [1. 0. 0.]\n [0. 1. 0.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 1. 0.]\n [0. 1. 0.]\n [0. 1. 0.]\n [1. 0. 0.]\n [0. 0. 1.]\n [1. 0. 0.]\n [0. 0. 1.]\n [1. 0. 0.]\n [1. 0. 0.]\n [1. 0. 0.]\n [0. 1. 0.]\n [0. 0. 1.]\n [1. 0. 0.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 1. 0.]\n [0. 1. 0.]\n [1. 0. 0.]\n [0. 1. 0.]\n [1. 0. 0.]\n [0. 1. 0.]\n [0. 1. 0.]\n [0. 1. 0.]\n [1. 0. 0.]\n [0. 0. 1.]\n [0. 1. 0.]\n [0. 1. 0.]\n [0. 1. 0.]\n [0. 0. 1.]\n [0. 1. 0.]\n [0. 1. 0.]\n [1. 0. 0.]\n [0. 0. 1.]\n [0. 1. 0.]\n [0. 1. 0.]\n [0. 1. 0.]\n [0. 1. 0.]\n [0. 1. 0.]\n [0. 1. 0.]\n [1. 0. 0.]\n [0. 1. 0.]\n [1. 0. 0.]\n [0. 1. 0.]\n [1. 0. 0.]\n [1. 0. 0.]\n [0. 0. 1.]\n [0. 1. 0.]\n [0. 0. 1.]\n [0. 0. 1.]\n [1. 0. 0.]\n [0. 0. 1.]\n [0. 1. 0.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 1. 0.]\n [1. 0. 0.]\n [0. 1. 0.]\n [0. 1. 0.]\n [0. 0. 1.]\n [1. 0. 0.]\n [1. 0. 0.]\n [0. 1. 0.]\n [1. 0. 0.]\n [1. 0. 0.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 1. 0.]\n [0. 0. 1.]\n [0. 1. 0.]\n [0. 1. 0.]\n [1. 0. 0.]\n [0. 0. 1.]\n [0. 1. 0.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 1. 0.]\n [0. 0. 1.]\n [0. 1. 0.]\n [0. 0. 1.]\n [0. 1. 0.]\n [1. 0. 0.]\n [0. 1. 0.]\n [0. 0. 1.]\n [0. 1. 0.]\n [0. 0. 1.]\n [1. 0. 0.]]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[[1. 0. 0.]\n [0. 1. 0.]\n [1. 0. 0.]]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\nprint(X_train.shape)\nprint(Y_train.shape)",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": "(120, 4)\n(120, 3)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def NeuralNetwork(X_train, Y_train, X_val=None, Y_val=None, epochs=10, nodes=[], lr=0.15):\n    hidden_layers = len(nodes)-1\n    weights = InitializeWeights(nodes)\n    for epoch in range(1, epochs+1):\n        weights=Train(X_train, Y_train, lr, weights)\n        if epoch%10==0:\n            print(\"Epoch {}\".format(epoch))\n            print(\"Training Accuracy:{}\".format(Accuracy(X_train, Y_train, weights)))\n    return weights",
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def InitializeWeights(nodes):\n    \"\"\"Initialize weights with random values in [0, 1] (including bias)\"\"\"\n    layers, weights = len(nodes), []\n    \n    for i in range(1, layers):\n        w = [[np.random.uniform(-1, 1) for k in range(nodes[i-1] + 1)]\n              for j in range(nodes[i])]\n        weights.append(np.matrix(w))\n    \n    return weights",
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def ForwardPropagation(x, weights, layers):\n    activations, layer_input = [x], x\n    for j in range(layers):\n        activation = Sigmoid(np.dot(layer_input, weights[j].T))\n        activations.append(activation)\n        layer_input = np.append(1, activation) # Augment with bias\n    \n    return activations",
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def BackPropagation(y, activations, weights, layers):\n    outputFinal = activations[-1]\n    error = np.matrix(y - outputFinal) # Error at output\n    \n    for j in range(layers, 0, -1):\n        currActivation = activations[j]\n        \n        if(j > 1):\n            # Augment previous activation\n            prevActivation = np.append(1, activations[j-1])\n        else:\n            # First hidden layer, prevActivation is input (without bias)\n            prevActivation = activations[0]\n        \n        delta = np.multiply(error, SigmoidDerivative(currActivation))\n        weights[j-1] += lr * np.multiply(delta.T, prevActivation)\n\n        w = np.delete(weights[j-1], [0], axis=1) # Remove bias from weights\n        error = np.dot(delta, w) # Calculate error for current layer\n    \n    return weights",
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def Train(X, Y, lr, weights):\n    layers = len(weights)\n    for i in range(len(X)):\n        x, y = X[i], Y[i]\n        x = np.matrix(np.append(1, x)) # Augment feature vector\n        \n        activations = ForwardPropagation(x, weights, layers)\n        weights = BackPropagation(y, activations, weights, layers)\n\n    return weights",
      "execution_count": 11,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def Sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef SigmoidDerivative(x):\n    return np.multiply(x, 1-x)",
      "execution_count": 12,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def Predict(item, weights):\n    layers = len(weights)\n    item = np.append(1, item) # Augment feature vector\n    \n    ##_Forward Propagation_##\n    activations = ForwardPropagation(item, weights, layers)\n    \n    outputFinal = activations[-1].A1\n    index = FindMaxActivation(outputFinal)\n\n    # Initialize prediction vector to zeros\n    y = [0 for i in range(len(outputFinal))]\n    y[index] = 1  # Set guessed class to 1\n\n    return y # Return prediction vector\n\n\ndef FindMaxActivation(output):\n    \"\"\"Find max activation in output\"\"\"\n    m, index = output[0], 0\n    for i in range(1, len(output)):\n        if(output[i] > m):\n            m, index = output[i], i\n    \n    return index",
      "execution_count": 13,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def Accuracy(X, Y, weights):\n    \"\"\"Run set through network, find overall accuracy\"\"\"\n    correct = 0\n\n    for i in range(len(X)):\n        x, y = X[i], list(Y[i])\n        guess = Predict(x, weights)\n\n        if(y == guess):\n            # Guessed correctly\n            correct += 1\n\n    return correct / len(X)",
      "execution_count": 14,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "f = len(X[0]) # Number of features\no = len(Y[0]) # Number of outputs / classes\nprint(f)\nprint(o)\nlayers = [4,10,10,3] # Number of nodes in layers\nlr, epochs = 0.15, 100\n\nweights = NeuralNetwork(X_train, Y_train, epochs=epochs, nodes=layers, lr=lr)",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": "4\n3\nEpoch 10\nTraining Accuracy:0.6416666666666667\nEpoch 20\nTraining Accuracy:0.7916666666666666\nEpoch 30\nTraining Accuracy:0.925\nEpoch 40\nTraining Accuracy:0.9833333333333333\nEpoch 50\nTraining Accuracy:0.975\nEpoch 60\nTraining Accuracy:0.975\nEpoch 70\nTraining Accuracy:0.975\nEpoch 80\nTraining Accuracy:0.975\nEpoch 90\nTraining Accuracy:0.975\nEpoch 100\nTraining Accuracy:0.975\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(\"Testing Accuracy: {}\".format(Accuracy(X_test, Y_test, weights)))",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Testing Accuracy: 0.9333333333333333\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}