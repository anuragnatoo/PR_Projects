{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>The input of the given question is 2Dimension images is resized to const size 75x75</h2>\n",
    "<h3>Then 2d array is flatten to 1d array and given as input to Perceptron and SVM(No visualisation can be done since weights are dim 5625x1) </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_size = (75,75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Resizing Image</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#resize image  to const size 75,75\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# cv2.imread(r'imagesq3/poly1.png',cv2.IMREAD_GRAYSCALE)\n",
    "# cv2.imread(r'imagesq3/poly2.png',cv2.IMREAD_GRAYSCALE)\n",
    "# cv2.imread(r'imagesq3/poly3.png',cv2.IMREAD_GRAYSCALE)\n",
    "# cv2.imread(r'imagesq3/poly4.png',cv2.IMREAD_GRAYSCALE)\n",
    "# cv2.imread(r'imagesq3/poly5.png',cv2.IMREAD_GRAYSCALE)\n",
    "# cv2.imread(r'imagesq3/poly6.png',cv2.IMREAD_GRAYSCALE)\n",
    "# cv2.imread(r'imagesq3/poly7.png',cv2.IMREAD_GRAYSCALE)\n",
    "# cv2.imread(r'imagesq3/poly8.png',cv2.IMREAD_GRAYSCALE)\n",
    "# cv2.imread(r'imagesq3/poly9.png',cv2.IMREAD_GRAYSCALE)\n",
    "# cv2.imread(r'imagesq3/poly10.png',cv2.IMREAD_GRAYSCALE)\n",
    "# cv2.imread(r'imagesq3/poly11.png',cv2.IMREAD_GRAYSCALE)\n",
    "# cv2.imread(r'imagesq3/poly12.png',cv2.IMREAD_GRAYSCALE)\n",
    "# cv2.imread(r'imagesq3/poly13.png',cv2.IMREAD_GRAYSCALE)\n",
    "# cv2.imread(r'imagesq3/poly14.png',cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Inew_img1 = Image.open(r'imagesq3/poly1.png')\n",
    "Inew_img1n = Inew_img1.resize(new_size) \n",
    "Inew_img1n.save(r'newimagesq3/poly1.png')\n",
    "Inew_img2 = Image.open(r'imagesq3/poly2.png')\n",
    "Inew_img2n = Inew_img1.resize(new_size)\n",
    "Inew_img2n.save(r'newimagesq3/poly2.png')\n",
    "Inew_img3 = Image.open(r'imagesq3/poly3.png')\n",
    "Inew_img3n = Inew_img3.resize(new_size) \n",
    "Inew_img3n.save(r'newimagesq3/poly3.png')\n",
    "Inew_img4 = Image.open(r'imagesq3/poly4.png')\n",
    "Inew_img4n = Inew_img4.resize(new_size) \n",
    "Inew_img4n.save(r'newimagesq3/poly4.png')\n",
    "Inew_img5 = Image.open(r'imagesq3/poly5.png')\n",
    "Inew_img5n = Inew_img5.resize(new_size) \n",
    "Inew_img5n.save(r'newimagesq3/poly5.png')\n",
    "Inew_img6 = Image.open(r'imagesq3/poly6.png')\n",
    "Inew_img6n = Inew_img6.resize(new_size) \n",
    "Inew_img6n.save(r'newimagesq3/poly6.png')\n",
    "Inew_img7 = Image.open(r'imagesq3/poly7.png')\n",
    "Inew_img7n = Inew_img7.resize(new_size) \n",
    "Inew_img7n.save(r'newimagesq3/poly7.png')\n",
    "Inew_img8 = Image.open(r'imagesq3/poly8.png')\n",
    "Inew_img8n = Inew_img8.resize(new_size) \n",
    "Inew_img8n.save(r'newimagesq3/poly8.png')\n",
    "Inew_img9 = Image.open(r'imagesq3/poly9.png')\n",
    "Inew_img9n = Inew_img9.resize(new_size) \n",
    "Inew_img9n.save(r'newimagesq3/poly9.png')\n",
    "Inew_img10 = Image.open(r'imagesq3/poly10.png')\n",
    "Inew_img10n = Inew_img10.resize(new_size) \n",
    "Inew_img10n.save(r'newimagesq3/poly10.png')\n",
    "Inew_img11 = Image.open(r'imagesq3/poly11.png')\n",
    "Inew_img11n = Inew_img11.resize(new_size) \n",
    "Inew_img11n.save(r'newimagesq3/poly11.png')\n",
    "Inew_img12 = Image.open(r'imagesq3/poly12.png')\n",
    "Inew_img12n = Inew_img12.resize(new_size) \n",
    "Inew_img12n.save(r'newimagesq3/poly12.png')\n",
    "Inew_img13 = Image.open(r'imagesq3/poly13.png')\n",
    "Inew_img13n = Inew_img13.resize(new_size) \n",
    "Inew_img13n.save(r'newimagesq3/poly13.png')\n",
    "Inew_img14 = Image.open(r'imagesq3/poly14.png')\n",
    "Inew_img14n = Inew_img14.resize(new_size) \n",
    "Inew_img14n.save(r'newimagesq3/poly14.png')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(217, 66)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Inew_img2.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75, 75)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Inew_img2n.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "\n",
    "# img1 = cv2.imread(r'imagesq3/poly1.png',cv2.IMREAD_GRAYSCALE)\n",
    "# img1 = cv2.imread(r'imagesq3/poly1.png',cv2.IMREAD_GRAYSCALE)\n",
    "# img2 = cv2.imread(r'imagesq3/poly2.png',cv2.IMREAD_GRAYSCALE)\n",
    "# img3 = cv2.imread(r'imagesq3/poly3.png',cv2.IMREAD_GRAYSCALE)\n",
    "# img4 = cv2.imread(r'imagesq3/poly4.png',cv2.IMREAD_GRAYSCALE)\n",
    "# img5 = cv2.imread(r'imagesq3/poly5.png',cv2.IMREAD_GRAYSCALE)\n",
    "# img6 = cv2.imread(r'imagesq3/poly6.png',cv2.IMREAD_GRAYSCALE)\n",
    "# img7 = cv2.imread(r'imagesq3/poly7.png',cv2.IMREAD_GRAYSCALE)\n",
    "# img8 = cv2.imread(r'imagesq3/poly8.png',cv2.IMREAD_GRAYSCALE)\n",
    "# img9 = cv2.imread(r'imagesq3/poly9.png',cv2.IMREAD_GRAYSCALE)\n",
    "# img10 = cv2.imread(r'imagesq3/poly10.png',cv2.IMREAD_GRAYSCALE)\n",
    "# img11 = cv2.imread(r'imagesq3/poly11.png',cv2.IMREAD_GRAYSCALE)\n",
    "# img12 = cv2.imread(r'imagesq3/poly12.png',cv2.IMREAD_GRAYSCALE)\n",
    "# img13 = cv2.imread(r'imagesq3/poly13.png',cv2.IMREAD_GRAYSCALE)\n",
    "# img14 = cv2.imread(r'imagesq3/poly14.png',cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# reading resized images\n",
    "\n",
    "img1 = cv2.imread(r'newimagesq3/poly1.png',cv2.IMREAD_GRAYSCALE)\n",
    "img2 = cv2.imread(r'newimagesq3/poly2.png',cv2.IMREAD_GRAYSCALE)\n",
    "img3 = cv2.imread(r'newimagesq3/poly3.png',cv2.IMREAD_GRAYSCALE)\n",
    "img4 = cv2.imread(r'newimagesq3/poly4.png',cv2.IMREAD_GRAYSCALE)\n",
    "img5 = cv2.imread(r'newimagesq3/poly5.png',cv2.IMREAD_GRAYSCALE)\n",
    "img6 = cv2.imread(r'newimagesq3/poly6.png',cv2.IMREAD_GRAYSCALE)\n",
    "img7 = cv2.imread(r'newimagesq3/poly7.png',cv2.IMREAD_GRAYSCALE)\n",
    "img8 = cv2.imread(r'newimagesq3/poly8.png',cv2.IMREAD_GRAYSCALE)\n",
    "img9 = cv2.imread(r'newimagesq3/poly9.png',cv2.IMREAD_GRAYSCALE)\n",
    "img10 = cv2.imread(r'newimagesq3/poly10.png',cv2.IMREAD_GRAYSCALE)\n",
    "img11 = cv2.imread(r'newimagesq3/poly11.png',cv2.IMREAD_GRAYSCALE)\n",
    "img12 = cv2.imread(r'newimagesq3/poly12.png',cv2.IMREAD_GRAYSCALE)\n",
    "img13 = cv2.imread(r'newimagesq3/poly13.png',cv2.IMREAD_GRAYSCALE)\n",
    "img14 = cv2.imread(r'newimagesq3/poly14.png',cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#list of images matrix\n",
    "\n",
    "\n",
    "img_list = [img1,\n",
    "img2,\n",
    "img3,\n",
    "img4,\n",
    "img5,\n",
    "img6,\n",
    "img7,\n",
    "img8,\n",
    "img9,\n",
    "img10,\n",
    "img11,\n",
    "img12,\n",
    "img13,\n",
    "img14]\n",
    "new_img_lit = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [0,0,0,0,0,0,0,1,1,1,1,1,1,1] #0 for class1 1 for class 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for imagesvar in img_list:\n",
    "#     print(len(imagesvar) ,\",\", len(imagesvar[0]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Flatening 75X75 to 5625X1 list </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5625"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatten_list = list(chain.from_iterable(img1)) \n",
    "len(flatten_list) #flatten list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten the matrix to 1d array\n",
    "dataset_forq3 = []\n",
    "for imagesvar in range(len(img_list)):\n",
    "    new_list = list(chain.from_iterable(img_list[imagesvar]))+[labels[imagesvar]]  #adding labels to data\n",
    "    dataset_forq3.append(new_list)\n",
    "#     print(len(imagesvar) ,\",\", len(imagesvar[0]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5626"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_forq3)\n",
    "len(dataset_forq3[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Running Tensorflow model of perceptron </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training\n",
    "# from tensorflow.keras import datasets, layers, models\n",
    "# import tensorflow\n",
    "# # keras = tf.keras\n",
    "# from tensorflow.keras.models import load_model\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, Flatten\n",
    "# from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Perceptron Class</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "from matplotlib import style\n",
    "import numpy as np \n",
    "\n",
    "\n",
    "style.use('ggplot')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def flatten(seq):\n",
    "  for el in seq:\n",
    "    if isinstance(el, list):\n",
    "      yield from flatten(el)\n",
    "    else:\n",
    "      yield el\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class PerceptronManual(object):\n",
    "    def __init__(self,traindata,random1 = True,*args):\n",
    "\n",
    "#         self.colors = {1:'r',-1:'b'}\n",
    "        intial_weight = 0.0\n",
    "        self.nptraindata = np.array(traindata)\n",
    "        self.weights = [intial_weight for i in range(len(traindata[0]))]\n",
    "        self.traindata = traindata\n",
    "        if random1 == False:\n",
    "            self.weights = [random.randint(1,100) for i in range(len(traindata[0]))]\n",
    "#             self.a = args[0]\n",
    "#             self.b = args[1]\n",
    "#             print(self.a)\n",
    "#             print(self.b)\n",
    "\n",
    "        #visualise\n",
    "       \n",
    "            \n",
    "        \n",
    "        \n",
    "        pass\n",
    "\n",
    "    \n",
    "    def initdraw(self,data1):\n",
    "        get_label = np.unique([i[-1] for i in data1])\n",
    "        self.colors= {}\n",
    "        if len(get_label)!=2:\n",
    "            self.visualise = None#if two categories\n",
    "        else:\n",
    "            self.colors[get_label[0]] = 'r'\n",
    "            self.colors[get_label[1]] = 'b'\n",
    "            \n",
    "            pass \n",
    "        if self.visualise is not None:\n",
    "            xaxis = list(self.nptraindata[:,0])\n",
    "            yaxis = list(self.nptraindata[:,1])\n",
    "            label1 = list(self.nptraindata[:,2])\n",
    "#             if data1:\n",
    "#             print(xaxis)\n",
    "#             print(yaxis)\n",
    "#             print(self.colors)\n",
    "            for ivar in range(len(label1)):\n",
    "                self.ax.scatter(xaxis[ivar],yaxis[ivar],s = 200,marker = '*', c = self.colors[label1[ivar]])\n",
    "                pass\n",
    "            \n",
    "        pass \n",
    "        \n",
    "\n",
    "    def predict(self,data1,weightstemp):\n",
    "        activation  = weightstemp[0]\n",
    "        for i in range(len(data1)-1):\n",
    "            activation += weightstemp[i+1]*data1[i]\n",
    "\n",
    "               \n",
    "            \n",
    "            \n",
    "        return 1.0 if activation>=0 else 0.0\n",
    "        pass \n",
    "\n",
    "        \n",
    "    def train_weights(self,l_rate,n_epoch):#tain data,learning data,epochs\n",
    "#         intial_weight = 0.0\n",
    "#         weights = [intial_weight for i in range(len(train[0]))]\n",
    "        self.iteration = 0\n",
    "        for epocvar in range(n_epoch):\n",
    "            sum_error_var =  0.0\n",
    "            for row in self.traindata:\n",
    "                prediction = self.predict(row,self.weights)\n",
    "                error_pre = row[-1]-prediction\n",
    "                sum_error_var+=error_pre**2\n",
    "                self.weights[0] = self.weights[0] + l_rate*error_pre\n",
    "                for ivar in range(len(row)-1):\n",
    "                    self.weights[ivar+1] = self.weights[ivar+1] +(l_rate*error_pre*row[ivar])\n",
    "\n",
    "            if sum_error_var!=0:\n",
    "                self.iteration+=1\n",
    "                pass\n",
    "            print(\"->Epoch = {:d},learning_rate = {:.3f},error = {:.3f}\".format(epocvar,l_rate,sum_error_var))\n",
    "#             print(\"Weights \",self.weights)\n",
    "        return self.weights \n",
    "        \n",
    "    def done(self,weightstemp):\n",
    "        self.visualise = None\n",
    "        if len(self.traindata[0])==3:\n",
    "            self.visualise = True\n",
    "            self.fig = plt.figure()\n",
    "            self.ax = self.fig.add_subplot(1,1,1)\n",
    "            self.initdraw(self.traindata)\n",
    "         #plt line\n",
    "        data1 = self.traindata\n",
    "        if self.visualise is not None and len(weightstemp)==3:\n",
    "            max_feature_value = (max(flatten(data1)))*1.1\n",
    "            min_feature_value = (min(flatten(data1)))*0.9\n",
    "            def hyperplane(x,w,b,v):\n",
    "                return (-w[0]*x-b+v) / w[1]\n",
    "            zer1 = hyperplane(min_feature_value,weightstemp[1:],weightstemp[0],0)\n",
    "            zer2 = hyperplane(max_feature_value,weightstemp[1:],weightstemp[0],0)\n",
    "            self.ax.plot([min_feature_value,max_feature_value],[zer1,zer2])\n",
    "            plt.show()\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create Perceptron object\n",
    "obkect = PerceptronManual(dataset_forq3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "->Epoch = 0,learning_rate = 0.010,error = 6.000\n",
      "->Epoch = 1,learning_rate = 0.010,error = 6.000\n",
      "->Epoch = 2,learning_rate = 0.010,error = 6.000\n",
      "->Epoch = 3,learning_rate = 0.010,error = 5.000\n",
      "->Epoch = 4,learning_rate = 0.010,error = 5.000\n",
      "->Epoch = 5,learning_rate = 0.010,error = 6.000\n",
      "->Epoch = 6,learning_rate = 0.010,error = 5.000\n",
      "->Epoch = 7,learning_rate = 0.010,error = 5.000\n",
      "->Epoch = 8,learning_rate = 0.010,error = 6.000\n",
      "->Epoch = 9,learning_rate = 0.010,error = 6.000\n",
      "->Epoch = 10,learning_rate = 0.010,error = 6.000\n",
      "->Epoch = 11,learning_rate = 0.010,error = 5.000\n",
      "->Epoch = 12,learning_rate = 0.010,error = 5.000\n",
      "->Epoch = 13,learning_rate = 0.010,error = 6.000\n",
      "->Epoch = 14,learning_rate = 0.010,error = 5.000\n",
      "->Epoch = 15,learning_rate = 0.010,error = 7.000\n",
      "->Epoch = 16,learning_rate = 0.010,error = 7.000\n",
      "->Epoch = 17,learning_rate = 0.010,error = 4.000\n",
      "->Epoch = 18,learning_rate = 0.010,error = 2.000\n",
      "->Epoch = 19,learning_rate = 0.010,error = 4.000\n",
      "->Epoch = 20,learning_rate = 0.010,error = 5.000\n",
      "->Epoch = 21,learning_rate = 0.010,error = 4.000\n",
      "->Epoch = 22,learning_rate = 0.010,error = 4.000\n",
      "->Epoch = 23,learning_rate = 0.010,error = 4.000\n",
      "->Epoch = 24,learning_rate = 0.010,error = 6.000\n",
      "->Epoch = 25,learning_rate = 0.010,error = 5.000\n",
      "->Epoch = 26,learning_rate = 0.010,error = 7.000\n",
      "->Epoch = 27,learning_rate = 0.010,error = 5.000\n",
      "->Epoch = 28,learning_rate = 0.010,error = 6.000\n",
      "->Epoch = 29,learning_rate = 0.010,error = 6.000\n",
      "->Epoch = 30,learning_rate = 0.010,error = 7.000\n",
      "->Epoch = 31,learning_rate = 0.010,error = 6.000\n",
      "->Epoch = 32,learning_rate = 0.010,error = 7.000\n",
      "->Epoch = 33,learning_rate = 0.010,error = 5.000\n",
      "->Epoch = 34,learning_rate = 0.010,error = 4.000\n",
      "->Epoch = 35,learning_rate = 0.010,error = 6.000\n",
      "->Epoch = 36,learning_rate = 0.010,error = 7.000\n",
      "->Epoch = 37,learning_rate = 0.010,error = 5.000\n",
      "->Epoch = 38,learning_rate = 0.010,error = 5.000\n",
      "->Epoch = 39,learning_rate = 0.010,error = 6.000\n",
      "->Epoch = 40,learning_rate = 0.010,error = 7.000\n",
      "->Epoch = 41,learning_rate = 0.010,error = 5.000\n",
      "->Epoch = 42,learning_rate = 0.010,error = 5.000\n",
      "->Epoch = 43,learning_rate = 0.010,error = 6.000\n",
      "->Epoch = 44,learning_rate = 0.010,error = 6.000\n",
      "->Epoch = 45,learning_rate = 0.010,error = 7.000\n",
      "->Epoch = 46,learning_rate = 0.010,error = 4.000\n",
      "->Epoch = 47,learning_rate = 0.010,error = 5.000\n",
      "->Epoch = 48,learning_rate = 0.010,error = 7.000\n",
      "->Epoch = 49,learning_rate = 0.010,error = 7.000\n",
      "->Epoch = 50,learning_rate = 0.010,error = 4.000\n",
      "->Epoch = 51,learning_rate = 0.010,error = 6.000\n",
      "->Epoch = 52,learning_rate = 0.010,error = 6.000\n",
      "->Epoch = 53,learning_rate = 0.010,error = 7.000\n",
      "->Epoch = 54,learning_rate = 0.010,error = 6.000\n",
      "->Epoch = 55,learning_rate = 0.010,error = 5.000\n",
      "->Epoch = 56,learning_rate = 0.010,error = 7.000\n",
      "->Epoch = 57,learning_rate = 0.010,error = 5.000\n",
      "->Epoch = 58,learning_rate = 0.010,error = 6.000\n",
      "->Epoch = 59,learning_rate = 0.010,error = 5.000\n",
      "->Epoch = 60,learning_rate = 0.010,error = 6.000\n",
      "->Epoch = 61,learning_rate = 0.010,error = 5.000\n",
      "->Epoch = 62,learning_rate = 0.010,error = 7.000\n",
      "->Epoch = 63,learning_rate = 0.010,error = 5.000\n",
      "->Epoch = 64,learning_rate = 0.010,error = 5.000\n",
      "->Epoch = 65,learning_rate = 0.010,error = 4.000\n",
      "->Epoch = 66,learning_rate = 0.010,error = 7.000\n",
      "->Epoch = 67,learning_rate = 0.010,error = 6.000\n",
      "->Epoch = 68,learning_rate = 0.010,error = 7.000\n",
      "->Epoch = 69,learning_rate = 0.010,error = 5.000\n",
      "->Epoch = 70,learning_rate = 0.010,error = 5.000\n",
      "->Epoch = 71,learning_rate = 0.010,error = 7.000\n",
      "->Epoch = 72,learning_rate = 0.010,error = 4.000\n",
      "->Epoch = 73,learning_rate = 0.010,error = 5.000\n",
      "->Epoch = 74,learning_rate = 0.010,error = 6.000\n",
      "->Epoch = 75,learning_rate = 0.010,error = 7.000\n",
      "->Epoch = 76,learning_rate = 0.010,error = 5.000\n",
      "->Epoch = 77,learning_rate = 0.010,error = 5.000\n",
      "->Epoch = 78,learning_rate = 0.010,error = 7.000\n",
      "->Epoch = 79,learning_rate = 0.010,error = 4.000\n",
      "->Epoch = 80,learning_rate = 0.010,error = 7.000\n",
      "->Epoch = 81,learning_rate = 0.010,error = 5.000\n",
      "->Epoch = 82,learning_rate = 0.010,error = 5.000\n",
      "->Epoch = 83,learning_rate = 0.010,error = 5.000\n",
      "->Epoch = 84,learning_rate = 0.010,error = 6.000\n",
      "->Epoch = 85,learning_rate = 0.010,error = 6.000\n",
      "->Epoch = 86,learning_rate = 0.010,error = 5.000\n",
      "->Epoch = 87,learning_rate = 0.010,error = 6.000\n",
      "->Epoch = 88,learning_rate = 0.010,error = 5.000\n",
      "->Epoch = 89,learning_rate = 0.010,error = 7.000\n",
      "->Epoch = 90,learning_rate = 0.010,error = 5.000\n",
      "->Epoch = 91,learning_rate = 0.010,error = 2.000\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "finalweights = obkect.train_weights(0.01,92)#learning rate = 0.01 and epochs = 92"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of iterations taken  92\n"
     ]
    }
   ],
   "source": [
    "print(\"The number of iterations taken \",obkect.iteration )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "obkect.done(finalweights) #no graph since 5625 weights are used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected=0, Predicted=0\n",
      "Expected=0, Predicted=0\n",
      "Expected=0, Predicted=0\n",
      "Expected=0, Predicted=0\n",
      "Expected=0, Predicted=0\n",
      "Expected=0, Predicted=0\n",
      "Expected=0, Predicted=0\n",
      "Expected=1, Predicted=1\n",
      "Expected=1, Predicted=1\n",
      "Expected=1, Predicted=0\n",
      "Expected=1, Predicted=0\n",
      "Expected=1, Predicted=1\n",
      "Expected=1, Predicted=1\n",
      "Expected=1, Predicted=1\n"
     ]
    }
   ],
   "source": [
    "for row in dataset_forq3:\n",
    "    prediction = obkect.predict(row, finalweights)\n",
    "    print(\"Expected=%d, Predicted=%d\" % (row[-1], prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default Perceptron class in tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(Conv2D(100, kernel_size=(2, 2), strides=(2, 2), activation='relu', input_shape=(75,75,1)))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# model.add(Flatten())\n",
    "\n",
    "# classifications = 2\n",
    "# model.add(Dense(classifications, activation='softmax'))\n",
    "# model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = []\n",
    "Ytrain = []\n",
    "for imagesvar in range(len(img_list)):\n",
    "    new_list = list(chain.from_iterable(img_list[imagesvar]))\n",
    "      #adding labels to data\n",
    "    Xtrain.append(new_list)\n",
    "#     print(len(imagesvar) ,\",\", len(imagesvar[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(Xtrain, labels, batch_size=250, epochs=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>S.V.M</h3><br/>\n",
    "<h4>Note :The scratch algorithm using quadprob cannot be used since weights are more than 2x1 </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Using existing S.V.M model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = svm.SVC()\n",
    "clf.fit(Xtrain, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing on train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict([Xtrain[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected=0, Predicted=0\n",
      "Expected=0, Predicted=0\n",
      "Expected=0, Predicted=0\n",
      "Expected=0, Predicted=1\n",
      "Expected=0, Predicted=1\n",
      "Expected=0, Predicted=1\n",
      "Expected=0, Predicted=0\n",
      "Expected=1, Predicted=1\n",
      "Expected=1, Predicted=1\n",
      "Expected=1, Predicted=1\n",
      "Expected=1, Predicted=1\n",
      "Expected=1, Predicted=0\n",
      "Expected=1, Predicted=1\n",
      "Expected=1, Predicted=1\n"
     ]
    }
   ],
   "source": [
    "counter1var = 0\n",
    "for row in Xtrain:\n",
    "    \n",
    "    prediction = clf.predict([row])\n",
    "    print(\"Expected=%d, Predicted=%d\" % (labels[counter1var], prediction))\n",
    "    counter1var+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[255., 255., 255., ..., 150., 154., 167.],\n",
       "       [  0.,   0.,   0., ..., 149., 149., 149.],\n",
       "       [  0.,   0.,   0., ...,  61.,  58.,   0.],\n",
       "       ...,\n",
       "       [ 42.,  41.,  41., ...,  41.,  41.,  41.],\n",
       "       [111.,  76.,  76., ...,  86.,  88., 128.],\n",
       "       [ 41.,  41.,  41., ...,  41.,  42.,  41.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.support_vectors_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
